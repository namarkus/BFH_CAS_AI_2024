{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namarkus/BFH_CAS_AI_2024/blob/main/Day17/torchrl_getting_started_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le8oFu_zI3-v"
      },
      "source": [
        "\n",
        "# Get started with TorchRL's modules\n",
        "\n",
        "**Author**: [Vincent Moens](https://github.com/vmoens)\n",
        "\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>To run this tutorial in a notebook, add an installation cell\n",
        "  at the beginning containing:\n",
        "\n",
        "```\n",
        "!pip install tensordict\n",
        "!pip install torchrl</p></div>\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gymnasium==0.29.1 torch torchrl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yP1XwplZHXQ",
        "outputId": "c4b3e044-010f-4b71-c7c7-54e15c6fcd37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting torchrl\n",
            "  Downloading torchrl-0.7.1-cp311-cp311-manylinux1_x86_64.whl.metadata (39 kB)\n",
            "Collecting tensordict\n",
            "  Downloading tensordict-0.7.1-cp311-cp311-manylinux1_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchrl) (24.2)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from tensordict) (3.10.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchrl-0.7.1-cp311-cp311-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensordict-0.7.1-cp311-cp311-manylinux1_x86_64.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, tensordict, torchrl\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.0.0\n",
            "    Uninstalling gymnasium-1.0.0:\n",
            "      Successfully uninstalled gymnasium-1.0.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 tensordict-0.7.1 torch-2.6.0 torchrl-0.7.1 triton-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bj2wp5VI3-0"
      },
      "source": [
        "Reinforcement Learning is designed to create policies that can effectively\n",
        "tackle specific tasks. Policies can take various forms, from a differentiable\n",
        "map transitioning from the observation space to the action space, to a more\n",
        "ad-hoc method like an argmax over a list of values computed for each possible\n",
        "action. Policies can be deterministic or stochastic, and may incorporate\n",
        "complex elements such as Recurrent Neural Networks (RNNs) or transformers.\n",
        "\n",
        "Accommodating all these scenarios can be quite intricate. In this succinct\n",
        "tutorial, we will delve into the core functionality of TorchRL in terms of\n",
        "policy construction. We will primarily focus on stochastic and Q-Value\n",
        "policies in two common scenarios: using a Multi-Layer Perceptron (MLP) or\n",
        "a Convolutional Neural Network (CNN) as backbones.\n",
        "\n",
        "## TensorDictModules\n",
        "\n",
        "Similar to how environments interact with instances of\n",
        ":class:`~tensordict.TensorDict`, the modules used to represent policies and\n",
        "value functions also do the same. The core idea is simple: encapsulate a\n",
        "standard :class:`~torch.nn.Module` (or any other function) within a class\n",
        "that knows which entries need to be read and passed to the module, and then\n",
        "records the results with the assigned entries. To illustrate this, we will\n",
        "use the simplest policy possible: a deterministic map from the observation\n",
        "space to the action space. For maximum generality, we will use a\n",
        ":class:`~torch.nn.LazyLinear` module with the Pendulum environment we\n",
        "instantiated in the previous tutorial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeG8SyZGI3-1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from tensordict.nn import TensorDictModule\n",
        "from torchrl.envs import GymEnv\n",
        "\n",
        "env = GymEnv(\"Pendulum-v1\")\n",
        "module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\n",
        "policy = TensorDictModule(\n",
        "    module,\n",
        "    in_keys=[\"observation\"], #\n",
        "    out_keys=[\"action\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBa5aGv9bYSY",
        "outputId": "fdb9baba-d3bf-4be3-cd13-9fc9fe81a558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorDictModule(\n",
              "    module=LazyLinear(in_features=0, out_features=1, bias=True),\n",
              "    device=cpu,\n",
              "    in_keys=['observation'],\n",
              "    out_keys=['action'])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ZkIUeWI3-3"
      },
      "source": [
        "This is all that's required to execute our policy! The use of a lazy module\n",
        "allows us to bypass the need to fetch the shape of the observation space, as\n",
        "the module will automatically determine it. This policy is now ready to be\n",
        "run in the environment:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5q0A0D5I3-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634f24da-48f5-44f6-a24a-fef39313c625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "            batch_size=torch.Size([10]),\n",
            "            device=None,\n",
            "            is_shared=False),\n",
            "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([10]),\n",
            "    device=None,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "rollout = env.rollout(max_steps=10, policy=policy)\n",
        "print(rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUZCPVO1I3-4"
      },
      "source": [
        "## Specialized wrappers\n",
        "\n",
        "To simplify the incorporation of :class:`~torch.nn.Module`s into your\n",
        "codebase, TorchRL offers a range of specialized wrappers designed to be\n",
        "used as actors, including :class:`~torchrl.modules.tensordict_module.Actor`,\n",
        "# :class:`~torchrl.modules.tensordict_module.ProbabilisticActor`,\n",
        "# :class:`~torchrl.modules.tensordict_module.ActorValueOperator` or\n",
        "# :class:`~torchrl.modules.tensordict_module.ActorCriticOperator`.\n",
        "For example, :class:`~torchrl.modules.tensordict_module.Actor` provides\n",
        "default values for the ``in_keys`` and ``out_keys``, making integration\n",
        "with many common environments straightforward:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoO5LtnGI3-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefee394-a5b1-467e-8ad9-eca08300487a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "            batch_size=torch.Size([10]),\n",
            "            device=None,\n",
            "            is_shared=False),\n",
            "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([10]),\n",
            "    device=None,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "from torchrl.modules.tensordict_module import Actor, ActorCriticOperator, ActorValueOperator, ProbabilisticActor\n",
        "\n",
        "\n",
        "policy = Actor(module)\n",
        "rollout = env.rollout(max_steps=10, policy=policy)\n",
        "print(rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF-2FPQlI3-5"
      },
      "source": [
        "The list of available specialized TensorDictModules is available in the\n",
        "`API reference <tdmodules>`.\n",
        "\n",
        "## Networks\n",
        "\n",
        "TorchRL also provides regular modules that can be used without recurring to\n",
        "tensordict features. The two most common networks you will encounter are\n",
        "the :class:`~torchrl.modules.MLP` and the :class:`~torchrl.modules.ConvNet`\n",
        "(CNN) modules. We can substitute our policy module with one of these:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-ElE7ARI3-5"
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import MLP\n",
        "\n",
        "module = MLP(\n",
        "    out_features=env.action_spec.shape[-1],\n",
        "    num_cells=[32, 64],\n",
        "    activation_class=torch.nn.Tanh,\n",
        ")\n",
        "policy = Actor(module)\n",
        "rollout = env.rollout(max_steps=10, policy=policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocYB3zpqI3-6"
      },
      "source": [
        "TorchRL also supports RNN-based policies. Since this is a more technical\n",
        "topic, it is treated in `a separate tutorial <RNN_tuto>`.\n",
        "\n",
        "## Probabilistic policies\n",
        "\n",
        "Policy-optimization algorithms like\n",
        "[PPO](https://arxiv.org/abs/1707.06347) require the policy to be\n",
        "stochastic: unlike in the examples above, the module now encodes a map from\n",
        "the observation space to a parameter space encoding a distribution over the\n",
        "possible actions. TorchRL facilitates the design of such modules by grouping\n",
        "under a single class the various operations such as building the distribution\n",
        "from the parameters, sampling from that distribution and retrieving the\n",
        "log-probability. Here, we'll be building an actor that relies on a regular\n",
        "normal distribution using three components:\n",
        "\n",
        "- An :class:`~torchrl.modules.MLP` backbone reading observations of size\n",
        "  ``[3]`` and outputting a single tensor of size ``[2]``;\n",
        "- A :class:`~tensordict.nn.distributions.NormalParamExtractor` module that\n",
        "  will split this output on two chunks, a mean and a standard deviation of\n",
        "  size ``[1]``;\n",
        "- A :class:`~torchrl.modules.tensordict_module.ProbabilisticActor` that will\n",
        "  read those parameters as ``in_keys``, create a distribution with them and\n",
        "  populate our tensordict with samples and log-probabilities.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDYW0yEeI3-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6629ef3-b9f2-470f-835d-9e76f1dedd60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        loc: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "            batch_size=torch.Size([10]),\n",
            "            device=None,\n",
            "            is_shared=False),\n",
            "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        sample_log_prob: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        scale: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([10]),\n",
            "    device=None,\n",
            "    is_shared=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensordict/nn/probabilistic.py:460: DeprecationWarning: You are querying the log-probability key of a SafeProbabilisticModule where the composite_lp_aggregate has not been set and the log-prob key has not been chosen. Currently, it is assumed that composite_lp_aggregate() will return True: the log-probs will be aggregated in a sample_log_prob entry. From v0.9, this behaviour will be changed and individual log-probs will be written in `('path', 'to', 'leaf', '<sample_name>_log_prob')`. To prepare for this change, call `set_composite_lp_aggregate(mode: bool).set()` at the beginning of your script (or set the COMPOSITE_LP_AGGREGATE env variable). Use mode=True to keep the current behaviour, and mode=False to use per-leaf log-probs.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensordict.nn.distributions import NormalParamExtractor\n",
        "from torch.distributions import Normal, Independent\n",
        "from torchrl.modules import ProbabilisticActor\n",
        "\n",
        "backbone = MLP(in_features=3, out_features=2)\n",
        "extractor = NormalParamExtractor()\n",
        "module = torch.nn.Sequential(backbone, extractor)\n",
        "td_module = TensorDictModule(module, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
        "policy = ProbabilisticActor(\n",
        "    td_module,\n",
        "    in_keys=[\"loc\", \"scale\"],\n",
        "    out_keys=[\"action\"],\n",
        "    distribution_class=Normal,\n",
        "    return_log_prob=True,\n",
        ")\n",
        "\n",
        "rollout = env.rollout(max_steps=10, policy=policy)\n",
        "print(rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPcS4jkEI3-6"
      },
      "source": [
        "There are a few things to note about this rollout:\n",
        "\n",
        "- Since we asked for it during the construction of the actor, the\n",
        "  log-probability of the actions given the distribution at that time is\n",
        "  also written. This is necessary for algorithms like PPO.\n",
        "- The parameters of the distribution are returned within the output\n",
        "  tensordict too under the ``\"loc\"`` and ``\"scale\"`` entries.\n",
        "\n",
        "You can control the sampling of the action to use the expected value or\n",
        "other properties of the distribution instead of using random samples if\n",
        "your application requires it. This can be controlled via the\n",
        ":func:`~torchrl.envs.utils.set_exploration_type` function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXZHbK7FI3-7"
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
        "\n",
        "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "    # takes the mean as action\n",
        "    rollout_mean = env.rollout(max_steps=10, policy=policy)\n",
        "with set_exploration_type(ExplorationType.RANDOM):\n",
        "    # Samples actions according to the dist\n",
        "    rollout_samples = env.rollout(max_steps=10, policy=policy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rollout_mean[\"action\"], rollout_samples[\"action\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8aZKXqtfKRb",
        "outputId": "723600f2-460e-40cf-bc67-2a8e09daae9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0957],\n",
              "         [-0.2097],\n",
              "         [-0.5096],\n",
              "         [-0.8056],\n",
              "         [-1.0961],\n",
              "         [-1.3751],\n",
              "         [-1.6316],\n",
              "         [-1.8497],\n",
              "         [-2.0113],\n",
              "         [-2.0995]], grad_fn=<StackBackward0>),\n",
              " tensor([[ 0.8380],\n",
              "         [-0.0074],\n",
              "         [-0.2659],\n",
              "         [ 0.4718],\n",
              "         [ 3.2400],\n",
              "         [-0.6625],\n",
              "         [ 0.2309],\n",
              "         [-0.3515],\n",
              "         [-0.2008],\n",
              "         [-0.8815]], grad_fn=<StackBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MdnjkrjI3-7"
      },
      "source": [
        "Check the ``default_interaction_type`` keyword argument in\n",
        "the docstrings to know more.\n",
        "\n",
        "## Exploration\n",
        "\n",
        "Stochastic policies like this somewhat naturally trade off exploration and\n",
        "exploitation, but deterministic policies won't. Fortunately, TorchRL can\n",
        "also palliate to this with its exploration modules.\n",
        "We will take the example of the :class:`~torchrl.modules.EGreedyModule`\n",
        "exploration module (check also\n",
        ":class:`~torchrl.modules.AdditiveGaussianModule` and\n",
        ":class:`~torchrl.modules.OrnsteinUhlenbeckProcessModule`).\n",
        "To see this module in action, let's revert to a deterministic policy:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdBA9hdzI3-7"
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictSequential\n",
        "from torchrl.modules import EGreedyModule\n",
        "\n",
        "policy = Actor(MLP(in_features=3, out_features=1, num_cells=[32, 64]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA8MjlqPI3-8"
      },
      "source": [
        "Our $\\epsilon$-greedy exploration module will usually be customized\n",
        "with a number of annealing frames and an initial value for the\n",
        "$\\epsilon$ parameter. A value of $\\epsilon = 1$ means that every\n",
        "action taken is random, while $\\epsilon=0$ means that there is no\n",
        "exploration at all. To anneal (i.e., decrease) the exploration factor, a call\n",
        "to :meth:`~torchrl.modules.EGreedyModule.step` is required (see the last\n",
        "`tutorial <gs_first_training>` for an example).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ByJ4cG-h7D-",
        "outputId": "23523237-2bb1-4783-c30e-7ecaeaab9a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GymEnv(env=Pendulum-v1, batch_size=torch.Size([]), device=None)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a-m1WplI3-8"
      },
      "outputs": [],
      "source": [
        "exploration_module = EGreedyModule(\n",
        "    spec=env.action_spec, annealing_num_steps=1000, eps_init=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KAazbiFI3-8"
      },
      "source": [
        "To build our explorative policy, we only had to concatenate the\n",
        "deterministic policy module with the exploration module within a\n",
        ":class:`~tensordict.nn.TensorDictSequential` module (which is the analogous\n",
        "to :class:`~torch.nn.Sequential` in the tensordict realm).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b20wwEcI3-9"
      },
      "outputs": [],
      "source": [
        "exploration_policy = TensorDictSequential(policy, exploration_module)\n",
        "\n",
        "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "    # Turns off exploration\n",
        "    rollout = env.rollout(max_steps=10, policy=exploration_policy)\n",
        "with set_exploration_type(ExplorationType.RANDOM):\n",
        "    # Turns on exploration\n",
        "    rollout = env.rollout(max_steps=10, policy=exploration_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8vP1saWI3-9"
      },
      "source": [
        "Because it must be able to sample random actions in the action space, the\n",
        ":class:`~torchrl.modules.EGreedyModule` must be equipped with the\n",
        "``action_space`` from the environment to know what strategy to use to\n",
        "sample actions randomly.\n",
        "\n",
        "## Q-Value actors\n",
        "\n",
        "In some settings, the policy isn't a standalone module but is constructed on\n",
        "top of another module. This is the case with **Q-Value actors**. In short, these\n",
        "actors require an estimate of the action value (most of the time discrete)\n",
        "and will greedily pick up the action with the highest value. In some\n",
        "settings (finite discrete action space and finite discrete state space),\n",
        "one can just store a 2D table of state-action pairs and pick up the\n",
        "action with the highest value. The innovation brought by\n",
        "[DQN](https://arxiv.org/abs/1312.5602) was to scale this up to continuous\n",
        "state spaces by utilizing a neural network to encode for the ``Q(s, a)``\n",
        "value map. Let's consider another environment with a discrete action space\n",
        "for a clearer understanding:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVgjvWLCI3-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca364b5-18d5-41f7-bad8-032bce2b8152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OneHot(\n",
            "    shape=torch.Size([2]),\n",
            "    space=CategoricalBox(n=2),\n",
            "    device=cpu,\n",
            "    dtype=torch.int64,\n",
            "    domain=discrete)\n"
          ]
        }
      ],
      "source": [
        "env = GymEnv(\"CartPole-v1\")\n",
        "print(env.action_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgDE6pIdI3-9"
      },
      "source": [
        "We build a value network that produces one value per action when it reads a\n",
        "state from the environment:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0ypW1WaI3--"
      },
      "outputs": [],
      "source": [
        "num_actions = 2\n",
        "value_net = TensorDictModule(\n",
        "    MLP(out_features=num_actions, num_cells=[32, 32]),\n",
        "    in_keys=[\"observation\"],\n",
        "    out_keys=[\"action_value\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F30NsJt0I3--"
      },
      "source": [
        "We can easily build our Q-Value actor by adding a\n",
        ":class:`~torchrl.modules.tensordict_module.QValueModule` after our value\n",
        "network:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCLxNCrvI3--"
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import QValueModule\n",
        "from tensordict.nn import TensorDictSequential\n",
        "\n",
        "policy = TensorDictSequential(\n",
        "    value_net,  # writes action values in our tensordict\n",
        "    QValueModule(spec=env.action_spec),  # Reads the \"action_value\" entry by default\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYlAz6d6I3--"
      },
      "source": [
        "Let's check it out! We run the policy for a couple of steps and look at the\n",
        "output. We should find an ``\"action_value\"`` as well as a\n",
        "``\"chosen_action_value\"`` entries in the rollout that we obtain:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cjAdK3GI3--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22980662-0fd5-4afd-da69-40132d0f4e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
            "        action_value: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        chosen_action_value: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "            batch_size=torch.Size([3]),\n",
            "            device=None,\n",
            "            is_shared=False),\n",
            "        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([3]),\n",
            "    device=None,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "rollout = env.rollout(max_steps=3, policy=policy)\n",
        "print(rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahoGFnuLI3-_"
      },
      "source": [
        "Because it relies on the ``argmax`` operator, this policy is deterministic.\n",
        "During data collection, we will need to explore the environment. For that,\n",
        "we are using the :class:`~torchrl.modules.tensordict_module.EGreedyModule`\n",
        "once again:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOCpyh9qI3-_"
      },
      "outputs": [],
      "source": [
        "policy_explore = TensorDictSequential(policy, EGreedyModule(env.action_spec))\n",
        "\n",
        "with set_exploration_type(ExplorationType.RANDOM):\n",
        "    rollout_explore = env.rollout(max_steps=3, policy=policy_explore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PpUahqEI3-_"
      },
      "source": [
        "This is it for our short tutorial on building a policy with TorchRL!\n",
        "\n",
        "There are many more things you can do with the library. A good place to start\n",
        "is to look at the `API reference for modules <ref_modules>`.\n",
        "\n",
        "Next steps:\n",
        "\n",
        "- Check how to use compound distributions with\n",
        "  :class:`~tensordict.nn.distributions.CompositeDistribution` when the\n",
        "  action is composite (e.g., a discrete and a continuous action are\n",
        "  required by the env);\n",
        "- Have a look at how you can use an RNN within the policy (a\n",
        "  `tutorial <RNN_tuto>`);\n",
        "- Compare this to the usage of transformers with the Decision Transformers\n",
        "  examples (see the ``example`` directory on GitHub).\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
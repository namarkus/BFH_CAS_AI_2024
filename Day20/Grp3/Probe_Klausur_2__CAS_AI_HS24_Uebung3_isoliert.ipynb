{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyFprCKICO34"
      },
      "source": [
        "# Eigene Probe-Klausur - Übung 3 isoliert\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaeRT9ptngE6"
      },
      "source": [
        "## 3 - Team 3 Deep Reinforcement Learning - Fehler Suche **50 Punkte**\n",
        "\n",
        "**Aufgabenstellung**: Fehler in der Rock-Paper-Scissors-Environment finden und beheben.\n",
        "Diese Vorlage enthält eine fehlerhafte Implementierung einer Rock-Paper-Scissors-Umgebung für ein Deep Reinforcement Learning (DRL)-Modell mit TorchRL.\n",
        "Eure Aufgabe ist es, die Fehler zu finden und zu beheben, damit das Modell korrekt funktioniert.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMPCXGHno8Kd"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torchrl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4bRxZEXpEZz"
      },
      "outputs": [],
      "source": [
        "  # Imports\n",
        "from typing import Optional\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "\n",
        "from tensordict import TensorDict, TensorDictBase\n",
        "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
        "\n",
        "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
        "from torchrl.data import OneHot, Composite, UnboundedContinuous, Categorical, Unbounded\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.objectives import DQNLoss, SoftUpdate\n",
        "from torchrl.envs import EnvBase, StepCounter, TransformedEnv, step_mdp\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnC3BPyDpImp"
      },
      "outputs": [],
      "source": [
        "# Aktionen und Rewards definieren\n",
        "ACTIONS = {0: \"Schere\", 1: \"Stein\", 2: \"Papier\"}\n",
        "REWARDS = {\n",
        "    (0, 0): 0, (0, 1): -1, (0, 2): 1,\n",
        "    (1, 0): 1, (1, 1): 0, (1, 2): -1,\n",
        "    (2, 0): -1, (2, 1): 1, (2, 2): 0\n",
        "}\n",
        "RANDOM = np.random.default_rng(42)\n",
        "def generate_random_moves(num_samples=10000):\n",
        "    data = {\n",
        "        # Unser Gegner hat leichten Hang zur Schere ;-)\n",
        "        \"move\": RANDOM.choice([0, 1, 2], size=num_samples, p=[0.4, 0.3, 0.3])\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Umgebung und ihr Verhalten definieren\n",
        "class RockPaperScissorsEnv(EnvBase):\n",
        "    def __init__(self, device=\"cpu\"):\n",
        "        super().__init__(device=device)\n",
        "        self.dataset = generate_random_moves()\n",
        "        self.num_features = 1\n",
        "        self.observation_spec = Composite(observation=Unbounded(shape=(1,), dtype=torch.float32))\n",
        "        self.action_spec = Composite(action=OneHot(n=len(ACTIONS.items()), dtype=torch.int64))\n",
        "        self.reward_spec = Composite(reward=Unbounded(shape=(1,), dtype=torch.float32))\n",
        "\n",
        "    def _step(self, tensordict):\n",
        "        action = tensordict[\"action\"].argmax(dim=-1).item()\n",
        "        next_sample = self.dataset.sample(1).iloc[0]\n",
        "        next_state = torch.tensor(\n",
        "            [next_sample[\"move\"]], dtype=torch.float32\n",
        "        )\n",
        "        reward = REWARDS[(action, next_sample[\"move\"])]\n",
        "        return TensorDict({\n",
        "            \"observation\": next_state,\n",
        "            \"reward\": torch.tensor([reward], dtype=torch.float32), # Fehler ind Testaufgabe: wurde dict als \"success\" im Tensordict ergänzt.\n",
        "            \"done\": torch.tensor([False], dtype=torch.bool),\n",
        "        }, batch_size=[])\n",
        "\n",
        "    def _reset(self, tensordict=None):\n",
        "        sample = self.dataset.sample(1).iloc[0]\n",
        "        state = torch.tensor([sample[\"move\"]], dtype=torch.float32)\n",
        "        return TensorDict({\n",
        "            \"observation\": state,\n",
        "        }, batch_size=[])\n",
        "\n",
        "    def _set_seed(self, seed: Optional[int]):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dajZAtB-pLjG"
      },
      "outputs": [],
      "source": [
        "env = RockPaperScissorsEnv()\n",
        "\n",
        "value_mlp = MLP(in_features=env.num_features, out_features=env.action_spec.shape[-1], num_cells=[64, 64])\n",
        "value_net = TensorDictModule(value_mlp, in_keys=[\"observation\"], out_keys=[\"action_value\"])\n",
        "policy = TensorDictSequential(value_net, QValueModule(spec=env.action_spec))\n",
        "exploration_module = EGreedyModule(\n",
        "    env.action_spec, annealing_num_steps=10_000, eps_init=0.9\n",
        ")\n",
        "policy_explore = TensorDictSequential(policy, exploration_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP3LpOpEpOss"
      },
      "outputs": [],
      "source": [
        "def create_env():\n",
        "  return TransformedEnv(RockPaperScissorsEnv(), StepCounter(max_steps=1_000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyOnUUfZpSQT"
      },
      "outputs": [],
      "source": [
        "init_rand_steps = 100\n",
        "collector = SyncDataCollector(\n",
        "    create_env_fn = create_env,\n",
        "    policy = policy_explore,\n",
        "    frames_per_batch=1,  # ✅ Match the batch size\n",
        "    total_frames=5_000,\n",
        "    init_random_frames=init_rand_steps,\n",
        "    storing_device=\"cpu\",  # ✅ Ensure storing happens on CPU\n",
        "    split_trajs=False,  # ✅ Prevent trajectory splitting from dropping keys\n",
        "    exploration_type=\"mode\"\n",
        ")\n",
        "rb = ReplayBuffer(storage=LazyTensorStorage(10_000))\n",
        "\n",
        "loss = DQNLoss(value_network=policy, action_space=env.action_spec, delay_value=True)\n",
        "optim = Adam(loss.parameters(), lr=0.02, weight_decay=1e-5)\n",
        "updater = SoftUpdate(loss, eps=0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3EecZlopViN"
      },
      "outputs": [],
      "source": [
        "total_count = 0\n",
        "optim_steps = 5\n",
        "batch_size = 100\n",
        "wins = 0\n",
        "draws = 0\n",
        "losses = 0\n",
        "\n",
        "for i, data in enumerate(collector):\n",
        "    rb.extend(data)\n",
        "    if len(rb) > init_rand_steps:\n",
        "        for _ in range(optim_steps):\n",
        "            sample = rb.sample(batch_size)\n",
        "            loss_vals = loss(sample)\n",
        "            loss_vals[\"loss\"].backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        # Update exploration factor and target params after optimization\n",
        "        exploration_module.step(data.numel())\n",
        "        updater.step()\n",
        "\n",
        "    # Update counters\n",
        "    total_count += data.numel()\n",
        "    rewards = data['next','reward'].view(-1).tolist()  # Extrahiere Rewards als Liste\n",
        "    wins += rewards.count(1)  # Zähle Gewinne\n",
        "    draws += rewards.count(0)  # Zähle Unentschieden\n",
        "    losses += rewards.count(-1)  # Zähle Verluste\n",
        "    if i % 100 == 0:\n",
        "        # Log progress every 100 iterations\n",
        "        print(\n",
        "            f\"Iteration {i}: Total steps: {total_count} - Wins: {wins}, Draws: {draws}, Losses: {losses} - Replay buffer size: {len(rb)}\"\n",
        "        )\n",
        "\n",
        "print(\n",
        "    f\"Finished after {total_count} steps.\"\n",
        "    f\"Final Results - Wins: {wins}, Draws: {draws}, Losses: {losses}\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
